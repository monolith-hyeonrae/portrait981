"""Highlight fusion for detecting photo-worthy moments."""

from typing import Optional, Dict, List
from collections import deque
from dataclasses import dataclass
import logging

from visualbase import Trigger

from vpx.sdk import Observation
from vpx.face_detect.types import FaceObservation
from visualpath.core.module import Module
from facemoment.algorithm.analyzers.face_classifier import FaceClassifierOutput
from facemoment.observability import ObservabilityHub, TraceLevel
from facemoment.observability.records import (
    GateChangeRecord,
    GateConditionRecord,
    TriggerDecisionRecord,
    TriggerFireRecord,
)

logger = logging.getLogger(__name__)


def _get_faces(obs):
    """Get faces from observation data."""
    if obs.data and hasattr(obs.data, 'faces'):
        return obs.data.faces
    return []


# Get the global observability hub
_hub = ObservabilityHub.get_instance()


@dataclass
class ExpressionState:
    """State for expression spike detection using EWMA."""

    ewma: float = 0.0
    ewma_var: float = 0.01  # Variance for z-score
    count: int = 0


@dataclass
class AdaptiveEmotionState:
    """Adaptive per-face happy tracking with baseline and recent values.

    Tracks happy emotion with:
    - baseline: Slow-moving average representing the person's "normal" happy level
    - recent: Fast-moving average representing current happy level
    - spike: recent - baseline, representing relative change from normal
    - spike_start_ns: When spike first exceeded threshold (for sustained detection)

    This allows detecting happy spikes relative to each person's baseline,
    not absolute thresholds.
    """

    # Baseline (slow EWMA, α≈0.02) - person's typical happy level
    baseline: float = 0.3

    # Recent (fast EWMA, α≈0.08) - current happy level (smoothed over ~1sec)
    recent: float = 0.3

    # Frame count for warmup
    count: int = 0

    # Spike sustain tracking
    spike_start_ns: Optional[int] = None  # When spike first exceeded threshold

    @property
    def spike(self) -> float:
        """Happy spike (recent - baseline)."""
        return self.recent - self.baseline


class HighlightFusion(Module):
    """Fusion module for detecting highlight-worthy moments.

    Combines signals from face, pose, and quality analyzers to
    identify moments worth capturing. Uses a quality gate with
    hysteresis and detects various trigger events.

    depends: ["face.detect"], optional_depends: ["face.expression", "face.classify"]

    Trigger Types:
    - expression_spike: Sudden increase in facial expression
    - head_turn: Quick head rotation (looking at camera)
    - hand_wave: Hand waving gesture
    - camera_gaze: Looking directly at camera (Phase 9)
    - passenger_interaction: Two people looking at each other (Phase 9)
    - gesture_vsign: V-sign gesture (Phase 9)
    - gesture_thumbsup: Thumbs up gesture (Phase 9)

    Gate Conditions (composition quality):
    - 1-2 faces detected
    - Face confidence above threshold
    - Face angles within limits
    - Face position centered
    - Quality metrics acceptable

    Args:
        face_conf_threshold: Minimum face confidence (default: 0.7).
        yaw_max: Maximum head yaw angle (default: 25.0).
        pitch_max: Maximum head pitch angle (default: 20.0).
        gate_open_duration_sec: Hysteresis duration to open gate (default: 0.7).
        gate_close_duration_sec: Hysteresis duration to close gate (default: 0.3).
        expression_z_threshold: Z-score threshold for expression spike (default: 2.0).
        ewma_alpha: EWMA smoothing factor (default: 0.1).
        head_turn_velocity_threshold: Angular velocity for head turn (default: 30.0 deg/sec).
        cooldown_sec: Cooldown between triggers (default: 2.0).
        consecutive_frames: Frames to confirm trigger (default: 2).
        pre_sec: Seconds before event in clip (default: 2.0).
        post_sec: Seconds after event in clip (default: 2.0).
        gaze_yaw_threshold: Max yaw for camera gaze detection (default: 10.0).
        gaze_pitch_threshold: Max pitch for camera gaze detection (default: 15.0).
        gaze_score_threshold: Min score to trigger camera gaze (default: 0.5).
        interaction_yaw_threshold: Min yaw for passenger interaction (default: 15.0).

    Example:
        >>> fusion = HighlightFusion()
        >>> for obs in observations:
        ...     result = fusion.process(frame, {"face_detect": obs})
        ...     if result.should_trigger:
        ...         print(f"Trigger: {result.trigger_reason}, score={result.trigger_score:.2f}")
    """

    # Dependency declaration - face_detect is required; others are best-effort
    depends = ["face.detect"]
    optional_depends = ["face.expression", "face.classify"]

    def __init__(
        self,
        # Gate thresholds
        face_conf_threshold: float = 0.7,
        yaw_max: float = 25.0,
        pitch_max: float = 20.0,
        min_face_area_ratio: float = 0.01,
        max_center_distance: float = 0.4,
        # Hysteresis
        gate_open_duration_sec: float = 0.7,
        gate_close_duration_sec: float = 0.3,
        # Expression detection (legacy z-score method)
        expression_z_threshold: float = 2.0,
        ewma_alpha: float = 0.1,
        # Adaptive emotion detection (new relative spike method)
        baseline_alpha: float = 0.02,  # Slow: person's typical emotion levels
        recent_alpha: float = 0.08,    # Smoothed over ~1sec at 10fps
        spike_threshold: float = 0.12, # Trigger when spike exceeds this
        spike_sustain_sec: float = 1.0, # Spike must sustain for this duration
        # Head turn detection
        head_turn_velocity_threshold: float = 30.0,
        # Timing
        cooldown_sec: float = 2.0,
        consecutive_frames: int = 2,
        pre_sec: float = 2.0,
        post_sec: float = 2.0,
        # Camera gaze detection (Phase 9)
        gaze_yaw_threshold: float = 10.0,
        gaze_pitch_threshold: float = 15.0,
        gaze_score_threshold: float = 0.5,
        # Passenger interaction detection (Phase 9)
        interaction_yaw_threshold: float = 15.0,
        # Main-only mode (Phase 16)
        main_only: bool = True,
    ):
        # Gate parameters
        self._face_conf_threshold = face_conf_threshold
        self._yaw_max = yaw_max
        self._pitch_max = pitch_max
        self._min_face_area = min_face_area_ratio
        self._max_center_dist = max_center_distance

        # Hysteresis (in nanoseconds)
        self._gate_open_duration_ns = int(gate_open_duration_sec * 1e9)
        self._gate_close_duration_ns = int(gate_close_duration_sec * 1e9)

        # Detection parameters (legacy)
        self._expr_z_threshold = expression_z_threshold
        self._ewma_alpha = ewma_alpha
        self._head_turn_vel_threshold = head_turn_velocity_threshold

        # Adaptive emotion detection parameters
        self._baseline_alpha = baseline_alpha
        self._recent_alpha = recent_alpha
        self._spike_threshold = spike_threshold
        self._spike_sustain_ns = int(spike_sustain_sec * 1e9)

        # Camera gaze detection parameters (Phase 9)
        self._gaze_yaw_threshold = gaze_yaw_threshold
        self._gaze_pitch_threshold = gaze_pitch_threshold
        self._gaze_score_threshold = gaze_score_threshold

        # Passenger interaction parameters (Phase 9)
        self._interaction_yaw_threshold = interaction_yaw_threshold

        # Main-only mode (Phase 16): only trigger on main face
        self._main_only = main_only
        self._main_face_id: Optional[int] = None  # Current main face ID

        # Timing
        self._cooldown_ns = int(cooldown_sec * 1e9)
        self._consecutive_required = consecutive_frames
        self._pre_sec = pre_sec
        self._post_sec = post_sec

        # State initialization
        self._reset_state()

    def _reset_state(self) -> None:
        """Reset all internal state."""
        # Gate state
        self._gate_open = False
        self._gate_condition_first_met_ns: Optional[int] = None
        self._gate_condition_first_failed_ns: Optional[int] = None

        # Cooldown
        self._last_trigger_ns: Optional[int] = None

        # Main face tracking (Phase 16)
        self._main_face_id: Optional[int] = None

        # Expression tracking (per face ID) - legacy z-score method
        self._expression_states: Dict[int, ExpressionState] = {}

        # Adaptive emotion tracking (per face ID) - new relative spike method
        self._adaptive_states: Dict[int, AdaptiveEmotionState] = {}

        # Head pose tracking (per face ID)
        self._prev_yaw: Dict[int, tuple[int, float]] = {}  # face_id -> (t_ns, yaw)

        # Consecutive frame counting
        self._consecutive_high: int = 0
        self._pending_trigger_reason: Optional[str] = None
        self._pending_trigger_score: float = 0.0

        # History
        self._recent_observations: deque[Observation] = deque(maxlen=30)
        self._observation_count = 0

    def update(
        self,
        observation: Observation,
        classifier_obs: Optional[Observation] = None,
    ) -> Observation:
        """Process observation and decide on trigger.

        Args:
            observation: New observation from analyzers.
            classifier_obs: Optional face classifier observation for main-only mode.

        Returns:
            Observation with trigger info in signals/metadata.
        """
        self._recent_observations.append(observation)
        self._observation_count += 1
        t_ns = observation.t_ns

        # Update main face ID from classifier or merged signals (Phase 16/17)
        self._update_main_face_id(classifier_obs, observation)

        # 1. Check cooldown
        if self._in_cooldown(t_ns):
            # Emit decision record at NORMAL level
            if _hub.enabled and _hub.is_level_enabled(TraceLevel.NORMAL):
                _hub.emit(TriggerDecisionRecord(
                    frame_id=observation.frame_id,
                    t_ns=t_ns,
                    gate_open=self._gate_open,
                    in_cooldown=True,
                    candidates=[],
                    consecutive_count=0,
                    consecutive_required=self._consecutive_required,
                    decision="blocked_cooldown",
                ))
            return Observation(
                source=self.name,
                frame_id=observation.frame_id,
                t_ns=t_ns,
                signals={
                    "should_trigger": False,
                    "trigger_score": 0.0,
                    "trigger_reason": "",
                    "observations_used": self._observation_count,
                },
                metadata={"state": "cooldown", "adaptive_summary": self.get_adaptive_summary()},
            )

        # 2. Update gate with hysteresis
        gate_conditions_met = self._check_gate_conditions(observation)
        self._update_gate_hysteresis(t_ns, gate_conditions_met, observation.frame_id)

        if not self._gate_open:
            self._consecutive_high = 0
            # Emit decision record at NORMAL level
            if _hub.enabled and _hub.is_level_enabled(TraceLevel.NORMAL):
                _hub.emit(TriggerDecisionRecord(
                    frame_id=observation.frame_id,
                    t_ns=t_ns,
                    gate_open=False,
                    in_cooldown=False,
                    candidates=[],
                    consecutive_count=0,
                    consecutive_required=self._consecutive_required,
                    decision="blocked_gate",
                ))
            return Observation(
                source=self.name,
                frame_id=observation.frame_id,
                t_ns=t_ns,
                signals={
                    "should_trigger": False,
                    "trigger_score": 0.0,
                    "trigger_reason": "",
                    "observations_used": self._observation_count,
                },
                metadata={
                    "state": "gate_closed",
                    "conditions_met": gate_conditions_met,
                    "adaptive_summary": self.get_adaptive_summary(),
                },
            )

        # 3. Detect trigger events and collect candidates
        trigger_reason = None
        trigger_score = 0.0
        candidates: List[Dict[str, any]] = []

        # Check expression spikes
        expr_spike = self._detect_expression_spike(observation)
        if expr_spike is not None:
            candidates.append({"reason": "expression_spike", "score": expr_spike, "source": "face"})
            trigger_reason = "expression_spike"
            trigger_score = expr_spike

        # Check head turns
        head_turn = self._detect_head_turn(observation)
        if head_turn is not None:
            candidates.append({"reason": "head_turn", "score": head_turn, "source": "face"})
            if trigger_reason is None or head_turn > trigger_score:
                trigger_reason = "head_turn"
                trigger_score = head_turn

        # Check hand waves (from pose analyzer)
        hand_wave = observation.signals.get("hand_wave_detected", 0.0)
        if hand_wave > 0.5:
            wave_score = observation.signals.get("hand_wave_confidence", 0.8)
            candidates.append({"reason": "hand_wave", "score": wave_score, "source": "body.pose"})
            if trigger_reason is None or wave_score > trigger_score:
                trigger_reason = "hand_wave"
                trigger_score = wave_score

        # Check camera gaze (Phase 9)
        gaze_detected, gaze_score = self._detect_camera_gaze(observation)
        if gaze_detected:
            candidates.append({"reason": "camera_gaze", "score": gaze_score, "source": "face"})
            if trigger_reason is None or gaze_score > trigger_score:
                trigger_reason = "camera_gaze"
                trigger_score = gaze_score

        # Check passenger interaction (Phase 9)
        interact_detected, interact_score = self._detect_passenger_interaction(observation)
        if interact_detected:
            candidates.append({"reason": "passenger_interaction", "score": interact_score, "source": "face"})
            if trigger_reason is None or interact_score > trigger_score:
                trigger_reason = "passenger_interaction"
                trigger_score = interact_score

        # Check gestures (Phase 9)
        gesture_reason, gesture_score = self._detect_gestures(observation)
        if gesture_reason:
            candidates.append({"reason": gesture_reason, "score": gesture_score, "source": "gesture"})
            if trigger_reason is None or gesture_score > trigger_score:
                trigger_reason = gesture_reason
                trigger_score = gesture_score

        # 4. Consecutive frame counting
        if trigger_reason is not None:
            if self._pending_trigger_reason == trigger_reason:
                self._consecutive_high += 1
                self._pending_trigger_score = max(self._pending_trigger_score, trigger_score)
            else:
                self._consecutive_high = 1
                self._pending_trigger_reason = trigger_reason
                self._pending_trigger_score = trigger_score
        else:
            self._consecutive_high = 0
            self._pending_trigger_reason = None
            self._pending_trigger_score = 0.0

        # 5. Fire trigger if consecutive threshold met
        if self._consecutive_high >= self._consecutive_required:
            self._last_trigger_ns = t_ns
            reason = self._pending_trigger_reason
            score = self._pending_trigger_score
            face_count = int(observation.signals.get("face_count", 0))

            # Reset consecutive counter
            self._consecutive_high = 0
            self._pending_trigger_reason = None
            self._pending_trigger_score = 0.0

            # Find event start time
            event_t_ns = self._find_event_start(reason, t_ns)

            trigger = Trigger.point(
                event_time_ns=event_t_ns,
                pre_sec=self._pre_sec,
                post_sec=self._post_sec,
                label="highlight",
                score=score,
                metadata={
                    "reason": reason,
                    "face_count": face_count,
                },
            )

            # Emit trigger decision and fire records
            if _hub.enabled:
                _hub.emit(TriggerDecisionRecord(
                    frame_id=observation.frame_id,
                    t_ns=t_ns,
                    gate_open=True,
                    in_cooldown=False,
                    candidates=candidates,
                    consecutive_count=self._consecutive_required,
                    consecutive_required=self._consecutive_required,
                    decision="triggered",
                    ewma_values={fid: s.ewma for fid, s in self._expression_states.items()},
                    ewma_vars={fid: s.ewma_var for fid, s in self._expression_states.items()},
                ))
                _hub.emit(TriggerFireRecord(
                    frame_id=observation.frame_id,
                    t_ns=t_ns,
                    event_t_ns=event_t_ns,
                    reason=reason,
                    score=score,
                    pre_sec=self._pre_sec,
                    post_sec=self._post_sec,
                    face_count=face_count,
                    consecutive_frames=self._consecutive_required,
                ))

            return Observation(
                source=self.name,
                frame_id=observation.frame_id,
                t_ns=t_ns,
                signals={
                    "should_trigger": True,
                    "trigger_score": score,
                    "trigger_reason": reason,
                    "observations_used": self._observation_count,
                },
                metadata={
                    "trigger": trigger,
                    "consecutive_frames": self._consecutive_required,
                    "adaptive_summary": self.get_adaptive_summary(),
                },
            )

        # Emit trigger decision record (no trigger)
        if _hub.enabled and _hub.is_level_enabled(TraceLevel.NORMAL):
            decision = "no_trigger"
            if candidates:
                decision = "consecutive_pending"
            _hub.emit(TriggerDecisionRecord(
                frame_id=observation.frame_id,
                t_ns=t_ns,
                gate_open=True,
                in_cooldown=False,
                candidates=candidates,
                consecutive_count=self._consecutive_high,
                consecutive_required=self._consecutive_required,
                decision=decision,
                ewma_values={fid: s.ewma for fid, s in self._expression_states.items()}
                    if _hub.is_level_enabled(TraceLevel.VERBOSE) else {},
                ewma_vars={fid: s.ewma_var for fid, s in self._expression_states.items()}
                    if _hub.is_level_enabled(TraceLevel.VERBOSE) else {},
            ))

        return Observation(
            source=self.name,
            frame_id=observation.frame_id,
            t_ns=t_ns,
            signals={
                "should_trigger": False,
                "trigger_score": 0.0,
                "trigger_reason": "",
                "observations_used": self._observation_count,
            },
            metadata={
                "state": "monitoring",
                "gate_open": self._gate_open,
                "consecutive_high": self._consecutive_high,
                "pending_reason": self._pending_trigger_reason,
                "adaptive_summary": self.get_adaptive_summary(),
            },
        )

    def _check_gate_conditions(self, observation: Observation) -> bool:
        """Check if gate conditions are met.

        Args:
            observation: Current observation.

        Returns:
            True if all gate conditions are satisfied.
        """
        faces = _get_faces(observation)
        face_count = int(observation.signals.get("face_count", len(faces)))

        # Track individual conditions for observability
        face_count_ok = 1 <= face_count <= 2
        quality_gate = observation.signals.get("quality_gate", 1.0)
        quality_ok = quality_gate >= 0.5

        # Initialize per-face checks
        confidence_ok = True
        inside_frame_ok = True
        yaw_ok = True
        pitch_ok = True
        area_ok = True
        center_ok = True

        max_confidence = 0.0
        max_yaw = 0.0
        max_pitch = 0.0

        # Check each face
        for face in faces:
            max_confidence = max(max_confidence, face.confidence)
            max_yaw = max(max_yaw, abs(face.yaw))
            max_pitch = max(max_pitch, abs(face.pitch))

            if face.confidence < self._face_conf_threshold:
                confidence_ok = False
            if not face.inside_frame:
                inside_frame_ok = False
            if abs(face.yaw) > self._yaw_max:
                yaw_ok = False
            if abs(face.pitch) > self._pitch_max:
                pitch_ok = False
            if face.area_ratio < self._min_face_area:
                area_ok = False
            if face.center_distance > self._max_center_dist:
                center_ok = False

        all_met = (
            face_count_ok and quality_ok and confidence_ok and
            inside_frame_ok and yaw_ok and pitch_ok and area_ok and center_ok
        )

        # Emit VERBOSE condition record
        if _hub.enabled and _hub.is_level_enabled(TraceLevel.VERBOSE):
            _hub.emit(GateConditionRecord(
                frame_id=observation.frame_id,
                gate_open=self._gate_open,
                face_count_ok=face_count_ok,
                confidence_ok=confidence_ok,
                yaw_ok=yaw_ok,
                pitch_ok=pitch_ok,
                inside_frame_ok=inside_frame_ok,
                area_ok=area_ok,
                center_ok=center_ok,
                quality_ok=quality_ok,
                face_count=face_count,
                max_confidence=max_confidence,
                max_yaw=max_yaw,
                max_pitch=max_pitch,
            ))

        return all_met

    def _update_gate_hysteresis(
        self, t_ns: int, conditions_met: bool, frame_id: int = 0
    ) -> None:
        """Update gate state with hysteresis.

        Args:
            t_ns: Current timestamp.
            conditions_met: Whether gate conditions are currently met.
            frame_id: Current frame ID for observability.
        """
        old_gate_open = self._gate_open

        if conditions_met:
            self._gate_condition_first_failed_ns = None

            if self._gate_open:
                # Already open, stay open
                pass
            else:
                # Track when conditions first met
                if self._gate_condition_first_met_ns is None:
                    self._gate_condition_first_met_ns = t_ns
                elif t_ns - self._gate_condition_first_met_ns >= self._gate_open_duration_ns:
                    # Conditions met long enough, open gate
                    self._gate_open = True
                    duration_ns = t_ns - self._gate_condition_first_met_ns
                    logger.debug(f"Gate opened at t={t_ns / 1e9:.3f}s")

                    # Emit gate change record
                    if _hub.enabled:
                        _hub.emit(GateChangeRecord(
                            frame_id=frame_id,
                            t_ns=t_ns,
                            old_state="closed",
                            new_state="open",
                            duration_ns=duration_ns,
                        ))
        else:
            self._gate_condition_first_met_ns = None

            if self._gate_open:
                # Track when conditions first failed
                if self._gate_condition_first_failed_ns is None:
                    self._gate_condition_first_failed_ns = t_ns
                elif t_ns - self._gate_condition_first_failed_ns >= self._gate_close_duration_ns:
                    # Conditions failed long enough, close gate
                    self._gate_open = False
                    duration_ns = t_ns - self._gate_condition_first_failed_ns
                    logger.debug(f"Gate closed at t={t_ns / 1e9:.3f}s")

                    # Emit gate change record
                    if _hub.enabled:
                        _hub.emit(GateChangeRecord(
                            frame_id=frame_id,
                            t_ns=t_ns,
                            old_state="open",
                            new_state="closed",
                            duration_ns=duration_ns,
                        ))

    def _detect_expression_spike(self, observation: Observation) -> Optional[float]:
        """Detect sustained happy spikes using adaptive baseline tracking.

        Uses per-face adaptive tracking:
        - baseline (slow EWMA): Person's typical happy level
        - recent (smoothed EWMA): Current happy level averaged over ~1sec
        - spike = recent - baseline: Relative change from normal
        - Spike must sustain above threshold for spike_sustain_sec

        This detects when someone's happy rises above THEIR normal level
        and STAYS elevated, filtering out brief fluctuations.

        Note: In main-only mode, only analyzes the main face.

        Args:
            observation: Current observation.

        Returns:
            Spike score if sustained spike detected, None otherwise.
        """
        t_ns = observation.t_ns
        max_spike_score = None

        # Get target faces (main only in main-only mode)
        target_faces = self._get_target_faces(observation)

        for face in target_faces:
            face_id = face.face_id

            # Get happy value from face signals
            happy = face.signals.get("em_happy", 0.0)

            # Get or create adaptive state
            if face_id not in self._adaptive_states:
                self._adaptive_states[face_id] = AdaptiveEmotionState(
                    baseline=happy,
                    recent=happy,
                )

            state = self._adaptive_states[face_id]

            # Update baseline (slow EWMA) - person's typical happy level
            state.baseline += self._baseline_alpha * (happy - state.baseline)

            # Update recent (smoothed EWMA) - current happy level
            state.recent += self._recent_alpha * (happy - state.recent)

            state.count += 1

            # Skip warmup period (need baseline to stabilize)
            if state.count < 10:
                continue

            # Check for spike (relative to baseline)
            spike = state.spike

            if spike > self._spike_threshold:
                # Track when spike started
                if state.spike_start_ns is None:
                    state.spike_start_ns = t_ns

                # Check if spike has sustained long enough
                spike_duration_ns = t_ns - state.spike_start_ns
                if spike_duration_ns >= self._spike_sustain_ns:
                    # Sustained spike! Calculate score
                    spike_score = min(1.0, spike / (self._spike_threshold * 2))
                    if max_spike_score is None or spike_score > max_spike_score:
                        max_spike_score = spike_score
            else:
                # Spike dropped below threshold, reset timer
                state.spike_start_ns = None

        return max_spike_score

    def _detect_head_turn(self, observation: Observation) -> Optional[float]:
        """Detect head turn events.

        Note: In main-only mode, only analyzes the main face.

        Args:
            observation: Current observation.

        Returns:
            Turn score if detected, None otherwise.
        """
        t_ns = observation.t_ns
        max_turn = None

        # Get target faces (main only in main-only mode)
        target_faces = self._get_target_faces(observation)

        for face in target_faces:
            face_id = face.face_id
            yaw = face.yaw

            if face_id in self._prev_yaw:
                prev_t_ns, prev_yaw = self._prev_yaw[face_id]
                dt_sec = (t_ns - prev_t_ns) / 1e9

                if dt_sec > 0 and dt_sec < 0.5:  # Reasonable time gap
                    # Compute angular velocity
                    angular_velocity = abs(yaw - prev_yaw) / dt_sec

                    if angular_velocity > self._head_turn_vel_threshold:
                        # Score based on velocity magnitude
                        turn_score = min(
                            1.0,
                            angular_velocity / (self._head_turn_vel_threshold * 2)
                        )
                        if max_turn is None or turn_score > max_turn:
                            max_turn = turn_score

            # Update previous yaw
            self._prev_yaw[face_id] = (t_ns, yaw)

        return max_turn

    def _detect_camera_gaze(self, observation: Observation) -> tuple[bool, float]:
        """Detect when subject is looking directly at camera.

        For gokart scenario, camera is mounted in front facing the driver.
        Looking at camera means yaw and pitch are close to 0.

        Note: In main-only mode, only analyzes the main face.

        Args:
            observation: Current observation.

        Returns:
            Tuple of (detected, score).
        """
        # Get target faces (main only in main-only mode)
        target_faces = self._get_target_faces(observation)

        if not target_faces:
            return False, 0.0

        max_score = 0.0
        detected = False

        for face in target_faces:
            # Calculate gaze score based on how centered the head pose is
            # yaw close to 0 = looking straight ahead at camera
            # pitch close to 0 = not looking up or down
            yaw_deviation = abs(face.yaw)
            pitch_deviation = abs(face.pitch)

            # Score decreases linearly as deviation increases
            yaw_score = max(0.0, 1.0 - yaw_deviation / self._gaze_yaw_threshold)
            pitch_score = max(0.0, 1.0 - pitch_deviation / self._gaze_pitch_threshold)

            # Combined score
            gaze_score = yaw_score * pitch_score

            if gaze_score > self._gaze_score_threshold:
                detected = True
                max_score = max(max_score, gaze_score)

        return detected, max_score

    def _detect_passenger_interaction(
        self, observation: Observation
    ) -> tuple[bool, float]:
        """Detect when two passengers are looking at each other.

        For gokart scenario with 2 passengers, detect when they turn
        to look at each other.

        Args:
            observation: Current observation.

        Returns:
            Tuple of (detected, score).
        """
        faces = _get_faces(observation)
        if len(faces) != 2:
            return False, 0.0

        f1, f2 = faces[0], faces[1]

        # Determine which face is on the left vs right
        # bbox[0] is normalized x coordinate (0=left, 1=right)
        if f1.bbox[0] < f2.bbox[0]:
            left_face, right_face = f1, f2
        else:
            left_face, right_face = f2, f1

        # Check if they're looking at each other:
        # - Left person should have positive yaw (looking right)
        # - Right person should have negative yaw (looking left)
        left_looking_right = left_face.yaw > self._interaction_yaw_threshold
        right_looking_left = right_face.yaw < -self._interaction_yaw_threshold

        if left_looking_right and right_looking_left:
            # Calculate interaction score based on yaw angles
            left_score = min(abs(left_face.yaw) / 45.0, 1.0)
            right_score = min(abs(right_face.yaw) / 45.0, 1.0)
            score = (left_score + right_score) / 2.0

            return True, score

        return False, 0.0

    def _detect_gestures(self, observation: Observation) -> tuple[Optional[str], float]:
        """Detect hand gestures from gesture analyzer signals.

        Looks for gesture signals in the observation (from GestureAnalyzer).

        Args:
            observation: Current observation.

        Returns:
            Tuple of (trigger_reason, score) or (None, 0.0).
        """
        # Check for gesture signals from GestureAnalyzer
        gesture_detected = observation.signals.get("gesture_detected", 0.0)
        if gesture_detected < 0.5:
            return None, 0.0

        gesture_type = observation.metadata.get("gesture_type", "")
        gesture_confidence = observation.signals.get("gesture_confidence", 0.0)

        # Map gesture types to trigger reasons
        gesture_triggers = {
            "v_sign": "gesture_vsign",
            "thumbs_up": "gesture_thumbsup",
            "ok_sign": "gesture_ok",
            "open_palm": "gesture_openpalm",
        }

        if gesture_type in gesture_triggers and gesture_confidence > 0.5:
            return gesture_triggers[gesture_type], gesture_confidence

        return None, 0.0

    def _find_event_start(self, reason: str, current_t_ns: int) -> int:
        """Find the start time of the trigger event.

        Args:
            reason: Trigger reason.
            current_t_ns: Current timestamp.

        Returns:
            Event start timestamp in nanoseconds.
        """
        # Look back through recent observations to find event start
        observations = list(self._recent_observations)

        if not observations:
            return current_t_ns

        for obs in reversed(observations[:-self._consecutive_required]):
            if reason == "expression_spike":
                max_expr = obs.signals.get("max_expression", 0)
                if max_expr < 0.5:
                    return obs.t_ns
            elif reason == "head_turn":
                # Find where angular velocity was low
                break
            elif reason == "hand_wave":
                wave = obs.signals.get("hand_wave_detected", 0)
                if wave < 0.5:
                    return obs.t_ns
            elif reason == "camera_gaze":
                # Find where gaze started
                detected, _ = self._detect_camera_gaze(obs)
                if not detected:
                    return obs.t_ns
            elif reason == "passenger_interaction":
                # Find where interaction started
                detected, _ = self._detect_passenger_interaction(obs)
                if not detected:
                    return obs.t_ns
            elif reason.startswith("gesture_"):
                # Find where gesture started
                gesture = obs.signals.get("gesture_detected", 0)
                if gesture < 0.5:
                    return obs.t_ns

        # Default to a bit before current time
        return observations[-min(3, len(observations))].t_ns

    def _in_cooldown(self, t_ns: int) -> bool:
        """Check if currently in cooldown.

        Args:
            t_ns: Current timestamp.

        Returns:
            True if in cooldown period.
        """
        if self._last_trigger_ns is None:
            return False
        return (t_ns - self._last_trigger_ns) < self._cooldown_ns

    def _update_main_face_id(
        self,
        classifier_obs: Optional[Observation],
        observation: Optional[Observation] = None,
    ) -> None:
        """Update main face ID from classifier observation or merged signals.

        Supports two modes:
        1. Explicit classifier_obs parameter (Phase 16)
        2. Merged observation with main_face_id in signals (Phase 17 Pathway)

        Args:
            classifier_obs: Face classifier observation.
            observation: Main observation (may contain main_face_id in signals).
        """
        # First, try explicit classifier observation
        if classifier_obs is not None and classifier_obs.data is not None:
            data = classifier_obs.data
            if hasattr(data, 'main_face') and data.main_face is not None:
                self._main_face_id = data.main_face.face.face_id
                return

        # Second, try merged observation signals (Pathway mode)
        if observation is not None and self._main_only:
            main_face_id = observation.signals.get("main_face_id")
            if main_face_id is not None:
                self._main_face_id = main_face_id

    def _get_target_faces(self, observation: Observation) -> List[FaceObservation]:
        """Get faces to analyze based on main-only mode.

        Args:
            observation: Current observation.

        Returns:
            List of faces to analyze (main only if main_only mode).
        """
        faces = _get_faces(observation)

        if not self._main_only or self._main_face_id is None:
            # Not in main-only mode or no main face identified yet
            return faces

        # Filter to only the main face
        main_faces = [f for f in faces if f.face_id == self._main_face_id]
        return main_faces if main_faces else faces  # Fallback to all if main not found

    def reset(self) -> None:
        """Reset fusion state."""
        self._reset_state()

    @property
    def name(self) -> str:
        """Module name."""
        return "highlight"

    stateful = True

    def process(self, frame, deps=None) -> Observation:
        """Process observations and decide on trigger (Module API).

        This is the unified Module interface. It extracts observations
        from deps and delegates to update().

        Args:
            frame: Current frame (used for timing if no observations).
            deps: Dict of observations from dependency modules.
                  Expected keys: "face.detect", "face.expression", "face.classify"

        Returns:
            Observation with trigger info in signals/metadata.
        """
        if not deps:
            return Observation(
                source=self.name,
                frame_id=getattr(frame, 'frame_id', 0),
                t_ns=getattr(frame, 't_src_ns', 0),
                signals={"should_trigger": False, "trigger_score": 0.0, "trigger_reason": ""},
            )

        # Get main observation (prefer "face.detect" or "face.expression")
        observation = deps.get("face.detect") or deps.get("face.expression")
        if observation is None:
            # Try any observation with faces
            for obs in deps.values():
                if hasattr(obs, 'signals'):
                    observation = obs
                    break

        if observation is None:
            return Observation(
                source=self.name,
                frame_id=getattr(frame, 'frame_id', 0),
                t_ns=getattr(frame, 't_src_ns', 0),
                signals={"should_trigger": False, "trigger_score": 0.0, "trigger_reason": ""},
            )

        # Get classifier observation if available
        classifier_obs = deps.get("face.classify")  # will be renamed in Phase 3

        return self.update(observation, classifier_obs)

    @property
    def is_gate_open(self) -> bool:
        return self._gate_open

    @property
    def in_cooldown(self) -> bool:
        return self._last_trigger_ns is not None

    @property
    def adaptive_states(self) -> Dict[int, AdaptiveEmotionState]:
        """Get adaptive emotion states for visualization."""
        return self._adaptive_states

    def get_adaptive_summary(self) -> Dict[str, any]:
        """Get summary of adaptive happy tracking for visualization.

        Returns:
            Dictionary with:
            - states: Per-face adaptive states (baseline, recent, spike, sustain_pct)
            - max_spike: Current maximum spike across all faces
            - threshold: Spike threshold for triggering
            - sustain_sec: Required sustain duration
        """
        if not self._adaptive_states:
            return {
                "states": {},
                "max_spike": 0.0,
                "threshold": self._spike_threshold,
                "sustain_sec": self._spike_sustain_ns / 1e9,
            }

        # Find face with maximum spike
        max_spike = 0.0
        max_sustain_pct = 0.0

        for face_id, state in self._adaptive_states.items():
            if state.count < 10:
                continue

            if state.spike > max_spike:
                max_spike = state.spike

        return {
            "states": {
                fid: {
                    "baseline": s.baseline,
                    "recent": s.recent,
                    "spike": s.spike,
                    "sustain_pct": 0.0 if s.spike_start_ns is None else 1.0,  # Simplified
                }
                for fid, s in self._adaptive_states.items()
            },
            "max_spike": max_spike,
            "threshold": self._spike_threshold,
            "sustain_sec": self._spike_sustain_ns / 1e9,
        }
